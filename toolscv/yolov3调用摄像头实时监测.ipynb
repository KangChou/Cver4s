{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\L\\.conda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\L\\.conda\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1247: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "model_data/yolo.h5 model, anchors, and classes loaded.\n",
      "WARNING:tensorflow:From C:\\Users\\L\\.conda\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1230: calling reduce_min_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 0.88 (303, 127) (621, 480)\n",
      "8.646727693000003\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 0.99 (225, 128) (588, 480)\n",
      "3.3624467839999994\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 0.98 (229, 126) (582, 480)\n",
      "3.062234024999995\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 0.97 (230, 128) (578, 478)\n",
      "2.9705837699999904\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 0.87 (63, 114) (591, 480)\n",
      "3.190770354999998\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "book 0.58 (185, 107) (382, 294)\n",
      "person 0.82 (236, 129) (588, 480)\n",
      "3.356410865000001\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "book 0.36 (187, 108) (382, 292)\n",
      "person 0.84 (234, 131) (595, 480)\n",
      "3.368477998000003\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "book 0.56 (186, 110) (381, 294)\n",
      "person 0.88 (123, 111) (621, 480)\n",
      "3.2371909920000093\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 0.95 (260, 135) (587, 479)\n",
      "3.031762342999997\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 0.87 (262, 138) (580, 476)\n",
      "3.0883372140000063\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 0.82 (213, 158) (527, 480)\n",
      "3.5112616450000047\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 0.85 (128, 151) (606, 480)\n",
      "3.5623946099999984\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "book 0.41 (186, 112) (385, 287)\n",
      "person 0.63 (212, 135) (519, 477)\n",
      "3.2245892909999867\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 0.78 (126, 112) (614, 480)\n",
      "3.068301163000001\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 0.97 (129, 85) (616, 480)\n",
      "2.924118656999994\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (178, 19) (635, 480)\n",
      "2.8941148289999887\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (176, 10) (639, 480)\n",
      "2.8841342240000074\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (182, 17) (634, 480)\n",
      "2.888354748000012\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 0.99 (201, 72) (640, 480)\n",
      "2.874368299999986\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (108, 9) (622, 480)\n",
      "2.8805932429999928\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (100, 53) (630, 480)\n",
      "3.002264432000004\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (103, 51) (623, 480)\n",
      "2.9233689780000134\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (96, 54) (630, 480)\n",
      "2.929900975999999\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (100, 52) (629, 480)\n",
      "2.8803665859999796\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (97, 58) (629, 480)\n",
      "2.888227735000015\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (52, 62) (613, 480)\n",
      "3.0314253520000136\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (100, 60) (629, 480)\n",
      "2.9650811369999985\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (92, 96) (546, 480)\n",
      "2.9871472650000044\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (57, 0) (599, 480)\n",
      "3.0535316790000024\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (70, 0) (591, 480)\n",
      "2.91706449199998\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (78, 0) (583, 480)\n",
      "3.237772175999993\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (69, 0) (587, 480)\n",
      "3.060155196000011\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (78, 0) (583, 480)\n",
      "3.0030970749999994\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (72, 0) (587, 480)\n",
      "3.049446721999999\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (55, 0) (598, 480)\n",
      "2.8563956830000166\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (99, 88) (539, 480)\n",
      "2.929339037999995\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (72, 0) (593, 480)\n",
      "3.0280943489999856\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (71, 0) (589, 480)\n",
      "2.952290839\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "person 1.00 (51, 0) (607, 480)\n",
      "3.885896588999998\n",
      "(416, 416, 3)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Class definition of YOLO_v3 style detection model on image and video\n",
    "\"\"\"\n",
    "import cv2\n",
    "import numpy\n",
    "import colorsys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "\n",
    "from yolo3.model import yolo_eval, yolo_body, tiny_yolo_body\n",
    "from yolo3.utils import letterbox_image\n",
    "import os\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "class YOLO(object):\n",
    "    _defaults = {\n",
    "        \"model_path\": 'model_data/yolo.h5',\n",
    "        \"anchors_path\": 'model_data/yolo_anchors.txt',\n",
    "        \"classes_path\": 'model_data/coco_classes.txt',\n",
    "        \"score\" : 0.3,\n",
    "        \"iou\" : 0.45,\n",
    "        \"model_image_size\" : (416, 416),\n",
    "        \"gpu_num\" : 1,\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_defaults(cls, n):\n",
    "        if n in cls._defaults:\n",
    "            return cls._defaults[n]\n",
    "        else:\n",
    "            return \"Unrecognized attribute name '\" + n + \"'\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(self._defaults) # set up default values\n",
    "        self.__dict__.update(kwargs) # and update with user overrides\n",
    "        self.class_names = self._get_class()\n",
    "        self.anchors = self._get_anchors()\n",
    "        self.sess = K.get_session()\n",
    "        self.boxes, self.scores, self.classes = self.generate()\n",
    "\n",
    "    def _get_class(self):\n",
    "        classes_path = os.path.expanduser(self.classes_path)\n",
    "        with open(classes_path) as f:\n",
    "            class_names = f.readlines()\n",
    "        class_names = [c.strip() for c in class_names]\n",
    "        return class_names\n",
    "\n",
    "    def _get_anchors(self):\n",
    "        anchors_path = os.path.expanduser(self.anchors_path)\n",
    "        with open(anchors_path) as f:\n",
    "            anchors = f.readline()\n",
    "        anchors = [float(x) for x in anchors.split(',')]\n",
    "        return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "    def generate(self):\n",
    "        model_path = os.path.expanduser(self.model_path)\n",
    "        assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.'\n",
    "\n",
    "        # Load model, or construct model and load weights.\n",
    "        num_anchors = len(self.anchors)\n",
    "        num_classes = len(self.class_names)\n",
    "        is_tiny_version = num_anchors==6 # default setting\n",
    "        try:\n",
    "            self.yolo_model = load_model(model_path, compile=False)\n",
    "        except:\n",
    "            self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \\\n",
    "                if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)\n",
    "            self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match\n",
    "        else:\n",
    "            assert self.yolo_model.layers[-1].output_shape[-1] == \\\n",
    "                num_anchors/len(self.yolo_model.output) * (num_classes + 5), \\\n",
    "                'Mismatch between model and given anchor and class sizes'\n",
    "\n",
    "        print('{} model, anchors, and classes loaded.'.format(model_path))\n",
    "\n",
    "        # Generate colors for drawing bounding boxes.\n",
    "        hsv_tuples = [(x / len(self.class_names), 1., 1.)\n",
    "                      for x in range(len(self.class_names))]\n",
    "        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "        self.colors = list(\n",
    "            map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\n",
    "                self.colors))\n",
    "        np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
    "        np.random.shuffle(self.colors)  # Shuffle colors to decorrelate adjacent classes.\n",
    "        np.random.seed(None)  # Reset seed to default.\n",
    "\n",
    "        # Generate output tensor targets for filtered bounding boxes.\n",
    "        self.input_image_shape = K.placeholder(shape=(2, ))\n",
    "        if self.gpu_num>=2:\n",
    "            self.yolo_model = multi_gpu_model(self.yolo_model, gpus=self.gpu_num)\n",
    "        boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors,\n",
    "                len(self.class_names), self.input_image_shape,\n",
    "                score_threshold=self.score, iou_threshold=self.iou)\n",
    "        return boxes, scores, classes\n",
    "\n",
    "    def detect_image(self, image):\n",
    "        start = timer()\n",
    "\n",
    "        if self.model_image_size != (None, None):\n",
    "            assert self.model_image_size[0]%32 == 0, 'Multiples of 32 required'\n",
    "            assert self.model_image_size[1]%32 == 0, 'Multiples of 32 required'\n",
    "            boxed_image = letterbox_image(image, tuple(reversed(self.model_image_size)))\n",
    "        else:\n",
    "            new_image_size = (image.width - (image.width % 32),\n",
    "                              image.height - (image.height % 32))\n",
    "            boxed_image = letterbox_image(image, new_image_size)\n",
    "        image_data = np.array(boxed_image, dtype='float32')\n",
    "\n",
    "        print(image_data.shape)\n",
    "        image_data /= 255.\n",
    "        image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
    "\n",
    "        out_boxes, out_scores, out_classes = self.sess.run(\n",
    "            [self.boxes, self.scores, self.classes],\n",
    "            feed_dict={\n",
    "                self.yolo_model.input: image_data,\n",
    "                self.input_image_shape: [image.size[1], image.size[0]],\n",
    "                K.learning_phase(): 0\n",
    "            })\n",
    "\n",
    "        print('Found {} boxes for {}'.format(len(out_boxes), 'img'))\n",
    "\n",
    "        font = ImageFont.truetype(font='font/FiraMono-Medium.otf',\n",
    "                    size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
    "        thickness = (image.size[0] + image.size[1]) // 300\n",
    "\n",
    "        for i, c in reversed(list(enumerate(out_classes))):\n",
    "            predicted_class = self.class_names[c]\n",
    "            box = out_boxes[i]\n",
    "            score = out_scores[i]\n",
    "\n",
    "            label = '{} {:.2f}'.format(predicted_class, score)\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            label_size = draw.textsize(label, font)\n",
    "\n",
    "            top, left, bottom, right = box\n",
    "            top = max(0, np.floor(top + 0.5).astype('int32'))\n",
    "            left = max(0, np.floor(left + 0.5).astype('int32'))\n",
    "            bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
    "            right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
    "            print(label, (left, top), (right, bottom))\n",
    "\n",
    "            if top - label_size[1] >= 0:\n",
    "                text_origin = np.array([left, top - label_size[1]])\n",
    "            else:\n",
    "                text_origin = np.array([left, top + 1])\n",
    "\n",
    "            # My kingdom for a good redistributable image drawing library.\n",
    "            for i in range(thickness):\n",
    "                draw.rectangle(\n",
    "                    [left + i, top + i, right - i, bottom - i],\n",
    "                    outline=self.colors[c])\n",
    "            draw.rectangle(\n",
    "                [tuple(text_origin), tuple(text_origin + label_size)],\n",
    "                fill=self.colors[c])\n",
    "            draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
    "            del draw\n",
    "\n",
    "        end = timer()\n",
    "        print(end - start)\n",
    "        return image\n",
    "\n",
    "    def close_session(self):\n",
    "        self.sess.close()\n",
    "\n",
    "def detect_video(yolo, video_path, output_path=\"\"):\n",
    "    import cv2\n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "    if not vid.isOpened():\n",
    "        raise IOError(\"Couldn't open webcam or video\")\n",
    "    video_FourCC    = int(vid.get(cv2.CAP_PROP_FOURCC))\n",
    "    video_fps       = vid.get(cv2.CAP_PROP_FPS)\n",
    "    video_size      = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "                        int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    isOutput = True if output_path != \"\" else False\n",
    "    if isOutput:\n",
    "        print(\"!!! TYPE:\", type(output_path), type(video_FourCC), type(video_fps), type(video_size))\n",
    "        out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size)\n",
    "    accum_time = 0\n",
    "    curr_fps = 0\n",
    "    fps = \"FPS: ??\"\n",
    "    prev_time = timer()\n",
    "    while True:\n",
    "        return_value, frame = vid.read()\n",
    "        image = Image.fromarray(frame)\n",
    "        image = yolo.detect_image(image)\n",
    "        result = np.asarray(image)\n",
    "        curr_time = timer()\n",
    "        exec_time = curr_time - prev_time\n",
    "        prev_time = curr_time\n",
    "        accum_time = accum_time + exec_time\n",
    "        curr_fps = curr_fps + 1\n",
    "        if accum_time > 1:\n",
    "            accum_time = accum_time - 1\n",
    "            fps = \"FPS: \" + str(curr_fps)\n",
    "            curr_fps = 0\n",
    "        cv2.putText(result, text=fps, org=(3, 15), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=0.50, color=(255, 0, 0), thickness=2)\n",
    "        cv2.namedWindow(\"result\", cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow(\"result\", result)\n",
    "        if isOutput:\n",
    "            out.write(result)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    yolo.close_session()\n",
    "if __name__ == '__main__':\n",
    "    detect_video(YOLO(), video_path=0, output_path=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
